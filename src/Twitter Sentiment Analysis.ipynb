{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis\n",
    "\n",
    "#### Goal: To create a model that can effectively predict sentiment (Positive or Negative) in tweets.\n",
    "\n",
    "Data is from Sentiment140 which provides 1.6 million labeled Tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan\n",
    "1. Take a small subset of the data in order to tune the XGB model\n",
    "2. Once the best parameters are chosen, will use grid search to tune the vectorizer.\n",
    "3. Increase the amount of data to train the model on.\n",
    "4. Perform Word2Vec on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### Imports ##################################\n",
    "\n",
    "# Basic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import re\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import boto3\n",
    "import io\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "import spacy\n",
    "spacy.load('en')\n",
    "from nltk.corpus import stopwords\n",
    "import preprocessor as p\n",
    "\n",
    "# Model Infrastructure\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 8.502373695373535 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#################################### Bring in Data #############################################\n",
    "start_time = time.time()\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "#Bring in Training Data\n",
    "obj = s3.get_object(Bucket='data-science-project-data', Key='Twitter_Sentiment_Analysis/training.1600000.processed.noemoticon.csv')\n",
    "cols = ['sentiment','id','date','query_string','user','text']\n",
    "tweets = pd.read_csv(io.BytesIO(obj['Body'].read()),header=None, names=cols, encoding = \"ISO-8859-1\")\n",
    "#train.set_index('bidder_id', inplace=True)\n",
    "\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query_string</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                          date query_string  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009     NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009     NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009     NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009     NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009     NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just Need the Sentiment and the Text\n",
    "tweets.drop(['id','date','query_string','user'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 74.30905294418335 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Clean the tweets\n",
    "p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.RESERVED, p.OPT.EMOJI, p.OPT.HASHTAG)\n",
    "def preprocess_tweet(tweet):\n",
    "    return p.clean(tweet)\n",
    "\n",
    "# Clean the tweets, by removing special characters\n",
    "start_time = time.time()\n",
    "tweets['Clean'] = tweets['text'].apply(lambda x: preprocess_tweet(x))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Down Sample\n",
    "tweets_subsampled_1, tweets_subsampled_2 = train_test_split(tweets, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split between outcome and Features\n",
    "y = tweets_subsampled_2['sentiment']\n",
    "X = tweets_subsampled_2['Clean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "Will use tuning to find the optimal parameters for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 69.3015489578247 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Create lemmatizer using spacy\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "\n",
    "def custom_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return([token.lemma_ for token in tokens if not token.is_punct])\n",
    "\n",
    "pipe = Pipeline(steps=[('vectidf', TfidfVectorizer(tokenizer=custom_tokenizer, stop_words='english',\n",
    "                                                   lowercase=True,use_idf=True,max_df=0.5,\n",
    "                                                  min_df=2, norm='l2', smooth_idf=True)),\n",
    "                 ('svd', TruncatedSVD(500)),\n",
    "                 ('norm',Normalizer(copy=False))\n",
    "                       ])\n",
    "\n",
    "tweets_transform = pipe.fit_transform(X)\n",
    "\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets_transform,y,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 41.16322708129883\n",
      "2 20.507357120513916\n",
      "3 13.794840574264526\n",
      "4 10.435452938079834\n"
     ]
    }
   ],
   "source": [
    "# Validate the multithreading is working\n",
    "# Note this test was down with only 0.05 of the data, and trucated to 100 SVD\n",
    "results = []\n",
    "num_jobs = [1, 2, 3, 4]\n",
    "for n in num_jobs:\n",
    "  start = time.time()\n",
    "  model = XGBClassifier(n_jobs=n)\n",
    "  model.fit(X_train, y_train)\n",
    "  elapsed = time.time() - start\n",
    "  print(n, elapsed)\n",
    "  results.append(elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.696\n",
      "-- Execution time: 89.55555629730225 seconds ---\n"
     ]
    }
   ],
   "source": [
    "####### Base Line Model ########\n",
    "warnings.filterwarnings('ignore')\n",
    "start_time = time.time()\n",
    "\n",
    "xgb_model = XGBClassifier(random_state=10) \n",
    "\n",
    "\n",
    "parameters = {'n_jobs':[-1],\n",
    "             }\n",
    "\n",
    "clf = GridSearchCV(xgb_model, parameters,cv=3, verbose=0,n_jobs=1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72365\n",
      "-- Execution time: 1408.1651225090027 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Tune n_estimators given the learning rate\n",
    "# n_estimators is the number of trees to use\n",
    "# learning_rate is to make the model more robust by shrinking the weights on each step. It determines the impact of each tree on the final outcome\n",
    "warnings.filterwarnings('ignore')\n",
    "start_time = time.time()\n",
    "xgb_model = XGBClassifier(learning_rate =0.1,\n",
    "                             max_depth=5,\n",
    "                             min_child_weight=1,\n",
    "                             gamma=0,\n",
    "                             subsample=0.8,\n",
    "                             colsample_bytree=0.8,\n",
    "                             scale_pos_weight=1,\n",
    "                             random_state=10) \n",
    " \n",
    "\n",
    "parameters = {'n_jobs':[30],\n",
    "             'n_estimators':range(100,1000,200)\n",
    "             }\n",
    "\n",
    "clf = GridSearchCV(xgb_model, parameters,cv=3, verbose=0,n_jobs=1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 500, 'n_jobs': 30}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find Best Parameters\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72605\n",
      "-- Execution time: 3803.7127678394318 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Tune Max_depth and min_child_weight\n",
    "# Max_depth is the maximum depth of the tree. Note the more, the more likelhood for overfitting\n",
    "# min_child_weight Defines the minimum sum of weights of all observations required in a child. its used to control overfitting as well\n",
    "warnings.filterwarnings('ignore')\n",
    "start_time = time.time()\n",
    "xgb_model = XGBClassifier(learning_rate =0.1,\n",
    "                             n_estimators=500,\n",
    "                             gamma=0,\n",
    "                             subsample=0.8,\n",
    "                             colsample_bytree=0.8,\n",
    "                             scale_pos_weight=1,\n",
    "                             random_state=10) \n",
    " \n",
    "\n",
    "parameters = {'n_jobs':[35],\n",
    "             'max_depth':range(3,10,2),\n",
    "             'min_child_weight':range(1,6,2)\n",
    "             }\n",
    "\n",
    "clf = GridSearchCV(xgb_model, parameters,cv=3, verbose=0,n_jobs=1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5, 'min_child_weight': 5, 'n_jobs': 35}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find Best Parameters\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.727625\n",
      "-- Execution time: 1406.7063238620758 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Tuned Gamma\n",
    "# Gamma specifies the minimum loss reduction required to make a split (on positive reduction in the loss function).\n",
    "warnings.filterwarnings('ignore')\n",
    "start_time = time.time()\n",
    "xgb_model = XGBClassifier(learning_rate =0.1,\n",
    "                             n_estimators=500,\n",
    "                             max_depth=5,\n",
    "                             min_child_weight=5,\n",
    "                             subsample=0.8,\n",
    "                             colsample_bytree=0.8,\n",
    "                             scale_pos_weight=1,\n",
    "                             random_state=10) \n",
    " \n",
    "\n",
    "parameters = {'n_jobs':[35],\n",
    "             'gamma':[i/10.0 for i in range(0,5)]\n",
    "             }\n",
    "\n",
    "clf = GridSearchCV(xgb_model, parameters,cv=3, verbose=0,n_jobs=1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gamma': 0.1, 'n_jobs': 35}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find Best Parameters\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.727625\n",
      "-- Execution time: 5729.693859577179 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Tune subsample and colsample_bytree\n",
    "# Subsample denotes the fraction of observates to be randomly sampled. Lower values make algorithm more conservative 2 small leads to underfitting\n",
    "# colsample_bytree denotes the fraction of coumns to be randomly sampled for each tree\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "start_time = time.time()\n",
    "xgb_model = XGBClassifier(learning_rate =0.1,\n",
    "                             n_estimators=500,\n",
    "                             max_depth=5,\n",
    "                             min_child_weight=5,\n",
    "                             gamma=0.1,\n",
    "                             scale_pos_weight=1,\n",
    "                             random_state=10) \n",
    " \n",
    "\n",
    "parameters = {'n_jobs':[200],\n",
    "             'subsample':[i/100.0 for i in range(75,90,5)],\n",
    "             'colsample_bytree':[i/100.0 for i in range(75,90,5)]\n",
    "             }\n",
    "\n",
    "clf = GridSearchCV(xgb_model, parameters,cv=3, verbose=0,n_jobs=1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.8, 'n_jobs': 200, 'subsample': 0.8}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find Best Parameters\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the regularization parameter\n",
    "# reg_alpha is the L1 regularization on the weights. Can be used with high dimensionality so it runs faster.\n",
    "# reg_lambda is the L2 regularization on the weights\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "start_time = time.time()\n",
    "\n",
    "xgb_model = XGBClassifier(learning_rate =0.1,\n",
    "                             n_estimators=500,\n",
    "                             max_depth=5,\n",
    "                             min_child_weight=5,\n",
    "                             gamma=0.1,\n",
    "                             subsample=0.8,\n",
    "                             colsample_bytree=0.8, \n",
    "                             scale_pos_weight=1,\n",
    "                             random_state=10) \n",
    " \n",
    "\n",
    "parameters = {'n_jobs':[-1],\n",
    "             'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05],\n",
    "             'reg_lambda':[0, 0.5, 1]\n",
    "             }\n",
    "\n",
    "clf = GridSearchCV(xgb_model, parameters,cv=3, verbose=0,n_jobs=1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Best Parameters\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune estimators and the learning parameter\n",
    "warnings.filterwarnings('ignore')\n",
    "start_time = time.time()\n",
    "xgb_model = XGBClassifier(   max_depth=5,\n",
    "                             min_child_weight=5,\n",
    "                             gamma=0.1,\n",
    "                             subsample=0.8,\n",
    "                             colsample_bytree=0.8, \n",
    "                             scale_pos_weight=1,\n",
    "                             random_state=10) \n",
    " \n",
    "\n",
    "parameters = {'n_jobs':[-1],\n",
    "             'n_estimators':range(500,5001,500),\n",
    "             'learning_rate':[0, 0.0001, 0.001, 0.01, 0.1, 0.2]\n",
    "             }\n",
    "\n",
    "clf = GridSearchCV(xgb_model, parameters,cv=3, verbose=0,n_jobs=1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizer Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Create lemmatizer using spacy\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "\n",
    "def custom_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return([token.lemma_ for token in tokens if not token.is_punct])\n",
    "\n",
    "xgb_model = XGBClassifier(   max_depth=5,\n",
    "                             min_child_weight=5,\n",
    "                             gamma=0.1,\n",
    "                             subsample=0.8,\n",
    "                             colsample_bytree=0.8, \n",
    "                             scale_pos_weight=1,\n",
    "                             random_state=10,\n",
    "                             n_jobs=200) \n",
    "\n",
    "pipe = Pipeline(steps=[('vectidf', TfidfVectorizer(tokenizer=custom_tokenizer, stop_words='english',lowercase=True,use_idf=True,max_df=0.5)),\n",
    "                 ('svd', TruncatedSVD(500)),\n",
    "                 #('norm',Normalizer(copy=False)),\n",
    "                 ('xgb',xgb_model)\n",
    "                 ])\n",
    "\n",
    "parameters = {'vectidf__ngram_range': [(1, 1), (1, 2)],\n",
    "              'vectidf__min_df':(1,2),\n",
    "              'vectidf__norm':['l1','l2']\n",
    "              }\n",
    "\n",
    "grid = GridSearchCV(pipe, parameters, n_jobs=1, cv=3, verbose=0)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Create lemmatizer using spacy\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "\n",
    "def custom_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return([token.lemma_ for token in tokens if not token.is_punct])\n",
    "\n",
    "xgb_model = XGBClassifier(   max_depth=5,\n",
    "                             min_child_weight=5,\n",
    "                             gamma=0.1,\n",
    "                             subsample=0.8,\n",
    "                             colsample_bytree=0.8, \n",
    "                             scale_pos_weight=1,\n",
    "                             random_state=10,\n",
    "                             n_jobs=200) \n",
    "\n",
    "pipe = Pipeline(steps=[('vectidf', TfidfVectorizer(tokenizer=custom_tokenizer, stop_words='english',lowercase=True,use_idf=True,max_df=0.5)),\n",
    "                 ('svd', TruncatedSVD(500)),\n",
    "                 #('norm',Normalizer(copy=False)),\n",
    "                 ('xgb',xgb_model)\n",
    "                 ])\n",
    "\n",
    "parameters = {'vectidf__ngram_range': [(1, 1)],\n",
    "              'vectidf__min_df':(1,2),\n",
    "              'vectidf__norm':['l1'],\n",
    "              'svd__n_components':[500, 1000, 1500, 2000]\n",
    "              }\n",
    "\n",
    "grid = GridSearchCV(pipe, parameters, n_jobs=1, cv=3, verbose=0)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split between outcome and Features\n",
    "y = tweets['sentiment']\n",
    "X = tweets['Clean']\n",
    "\n",
    "#splitting into training and test sets even though still going to do k folds on the training data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "\n",
    "# Going to remove the normalizer step since the tfidfvectoizer already normalizes, and the model doesnt need it to be normalized.\n",
    "\n",
    "start_time = time.time()\n",
    "# Create lemmatizer using spacy\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "\n",
    "def custom_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return([token.lemma_ for token in tokens if not token.is_punct])\n",
    "\n",
    "xgb_model = XGBClassifier(   max_depth=5,\n",
    "                             min_child_weight=5,\n",
    "                             gamma=0.1,\n",
    "                             subsample=0.8,\n",
    "                             colsample_bytree=0.8, \n",
    "                             scale_pos_weight=1,\n",
    "                             random_state=10,\n",
    "                             n_jobs=-1) \n",
    "\n",
    "pipe = Pipeline(steps=[('vectidf', TfidfVectorizer(tokenizer=custom_tokenizer, stop_words='english',lowercase=True,use_idf=True,max_df=0.5)),\n",
    "                 ('svd', TruncatedSVD(2000)),\n",
    "                 ('xgb',xgb_model)\n",
    "                 ])\n",
    "\n",
    "parameters = {'vectidf__ngram_range': [(1, 1), (1, 2)],\n",
    "              'vectidf__min_df':(1,2),\n",
    "              'vectidf__norm':['l1','l2'],\n",
    "              'vectidf__smooth_idf':[True, False]\n",
    "              }\n",
    "# Pre dispatch controls the number of jobs that gets dispatched during parallel execution. \n",
    "grid = GridSearchCV(pipe, parameters, pre_dispatch=3, cv=3, verbose=0)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
