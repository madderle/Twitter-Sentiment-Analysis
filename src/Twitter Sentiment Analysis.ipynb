{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis\n",
    "\n",
    "#### Goal: To create a model that can effectively predict sentiment (Positive or Negative) in tweets.\n",
    "\n",
    "Data is from Sentiment140 which provides 1.6 million labeled Tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan\n",
    "1. Take a small subset of the data in order to tune the XGB model\n",
    "2. Once the best parameters are chosen, will use grid search to tune the vectorizer.\n",
    "3. Increase the amount of data to train the model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### Imports ##################################\n",
    "\n",
    "# Basic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import re\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import boto3\n",
    "import io\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "import spacy\n",
    "spacy.load('en')\n",
    "from nltk.corpus import stopwords\n",
    "import preprocessor as p\n",
    "\n",
    "# Model Infrastructure\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import linear_model\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 6.1940016746521 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#################################### Bring in Data #############################################\n",
    "start_time = time.time()\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "#Bring in Training Data\n",
    "obj = s3.get_object(Bucket='data-science-project-data', Key='Twitter_Sentiment_Analysis/training.1600000.processed.noemoticon.csv')\n",
    "cols = ['sentiment','id','date','query_string','user','text']\n",
    "tweets = pd.read_csv(io.BytesIO(obj['Body'].read()),header=None, names=cols, encoding = \"ISO-8859-1\")\n",
    "#train.set_index('bidder_id', inplace=True)\n",
    "\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query_string</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                          date query_string  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009     NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009     NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009     NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009     NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009     NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just Need the Sentiment and the Text\n",
    "tweets.drop(['id','date','query_string','user'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 54.30187749862671 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Clean the tweets\n",
    "p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.RESERVED, p.OPT.EMOJI, p.OPT.HASHTAG)\n",
    "def preprocess_tweet(tweet):\n",
    "    return p.clean(tweet)\n",
    "\n",
    "# Clean the tweets, by removing special characters\n",
    "start_time = time.time()\n",
    "tweets['Clean'] = tweets['text'].apply(lambda x: preprocess_tweet(x))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Down Sample\n",
    "tweets_subsampled_1, tweets_subsampled_2 = train_test_split(tweets, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split between outcome and Features\n",
    "y = tweets_subsampled_2['sentiment']\n",
    "X = tweets_subsampled_2['Clean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "Will use tuning to find the optimal parameters for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing Finished. Number of features: 10000\n",
      "-- Execution time: 19.35665512084961 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Create lemmatizer using spacy\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "\n",
    "def custom_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return([token.lemma_ for token in tokens if not token.is_punct])\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=custom_tokenizer, stop_words='english',\n",
    "                             lowercase=True, use_idf=True, max_df=0.5, max_features=10000,\n",
    "                             min_df=2, norm='l2', smooth_idf=True, ngram_range=(1, 2))\n",
    "\n",
    "tweets_tfidf = vectorizer.fit_transform(X)\n",
    "print(\"Vectorizing Finished. Number of features: %d\" % tweets_tfidf.get_shape()[1])\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 597.3474566936493 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "                 ('svd', TruncatedSVD(5000)),\n",
    "                 ('norm',Normalizer(copy=False))\n",
    "                       ])\n",
    "\n",
    "tweets_transform = pipe.fit_transform(tweets_tfidf)\n",
    "\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8685416919446195"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How much of the variance does the svd explain? With max_features NOT set\n",
    "# 500 componenets explaines 47% of variance\n",
    "# 1000 components explains 59% of variance\n",
    "# 2000 compoenents explains 71% of variance\n",
    "# 3000 components explaines 77% of variance\n",
    "# 4000 components explains 82% of variance\n",
    "# With MAX Features to 10,000\n",
    "# 500 explains 43% of data, but its Much faster (34 seconds)\n",
    "# 5000 explains 86% of the variance at 597 seconds. \n",
    "pipe.get_params()['svd'].explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(tweets_transform,y,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 41.16322708129883\n",
      "2 20.507357120513916\n",
      "3 13.794840574264526\n",
      "4 10.435452938079834\n"
     ]
    }
   ],
   "source": [
    "# Validate the multithreading is working\n",
    "# Note this test was down with only 0.05 of the data, and trucated to 100 SVD\n",
    "results = []\n",
    "num_jobs = [1, 2, 3, 4]\n",
    "for n in num_jobs:\n",
    "  start = time.time()\n",
    "  model = XGBClassifier(n_jobs=n)\n",
    "  model.fit(X_train, y_train)\n",
    "  elapsed = time.time() - start\n",
    "  print(n, elapsed)\n",
    "  results.append(elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Score: 0.736\n",
      "-- Execution time: 711.132809638977 seconds ---\n"
     ]
    }
   ],
   "source": [
    "###### Test the max features of 10,000 and SVD of 5000 components #########\n",
    "warnings.filterwarnings('ignore')\n",
    "start_time = time.time()\n",
    "xgb_model = XGBClassifier(learning_rate =0.1,\n",
    "                             max_depth=5,\n",
    "                             min_child_weight=1,\n",
    "                             gamma=0,\n",
    "                             n_estimators=500, \n",
    "                             subsample=0.8,\n",
    "                             colsample_bytree=0.8,\n",
    "                             scale_pos_weight=1,\n",
    "                             random_state=10,\n",
    "                             n_jobs=-1)\n",
    "\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Test Set Score: \" + str(xgb_model.score(X_test, y_test)))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- CV Score: 0.6926416666666667\n",
      "-- Test Set Score: 0.690625\n",
      "-- Execution time: 34.98430633544922 seconds ---\n"
     ]
    }
   ],
   "source": [
    "####### Base Line Model ########\n",
    "####### Didnt run this cell again for documentation purposes. This ran before used maxed features #######\n",
    "warnings.filterwarnings('ignore')\n",
    "start_time = time.time()\n",
    "\n",
    "xgb_model = XGBClassifier(random_state=10) \n",
    "\n",
    "\n",
    "parameters = {'n_jobs':[-1],\n",
    "             }\n",
    "\n",
    "clf = GridSearchCV(xgb_model, parameters,cv=3, verbose=0,n_jobs=1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"-- CV Score: \" + str(clf.best_score_))\n",
    "print(\"-- Test Set Score: \" + str(clf.score(X_test, y_test)))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72365\n",
      "-- Execution time: 1408.1651225090027 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Tune n_estimators given the learning rate\n",
    "# n_estimators is the number of trees to use\n",
    "# learning_rate is to make the model more robust by shrinking the weights on each step. It determines the impact of each tree on the final outcome\n",
    "warnings.filterwarnings('ignore')\n",
    "start_time = time.time()\n",
    "xgb_model = XGBClassifier(learning_rate =0.1,\n",
    "                             max_depth=5,\n",
    "                             min_child_weight=1,\n",
    "                             gamma=0,\n",
    "                             subsample=0.8,\n",
    "                             colsample_bytree=0.8,\n",
    "                             scale_pos_weight=1,\n",
    "                             random_state=10) \n",
    " \n",
    "\n",
    "parameters = {'n_jobs':[30],\n",
    "             'n_estimators':range(100,1000,200)\n",
    "             }\n",
    "\n",
    "clf = GridSearchCV(xgb_model, parameters,cv=3, verbose=0,n_jobs=1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 500, 'n_jobs': 30}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find Best Parameters\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72605\n",
      "-- Execution time: 3803.7127678394318 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Tune Max_depth and min_child_weight\n",
    "# Max_depth is the maximum depth of the tree. Note the more, the more likelhood for overfitting\n",
    "# min_child_weight Defines the minimum sum of weights of all observations required in a child. its used to control overfitting as well\n",
    "warnings.filterwarnings('ignore')\n",
    "start_time = time.time()\n",
    "xgb_model = XGBClassifier(learning_rate =0.1,\n",
    "                             n_estimators=500,\n",
    "                             gamma=0,\n",
    "                             subsample=0.8,\n",
    "                             colsample_bytree=0.8,\n",
    "                             scale_pos_weight=1,\n",
    "                             random_state=10) \n",
    " \n",
    "\n",
    "parameters = {'n_jobs':[35],\n",
    "             'max_depth':range(3,10,2),\n",
    "             'min_child_weight':range(1,6,2)\n",
    "             }\n",
    "\n",
    "clf = GridSearchCV(xgb_model, parameters,cv=3, verbose=0,n_jobs=1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5, 'min_child_weight': 5, 'n_jobs': 35}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find Best Parameters\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.727625\n",
      "-- Execution time: 1406.7063238620758 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Tuned Gamma\n",
    "# Gamma specifies the minimum loss reduction required to make a split (on positive reduction in the loss function).\n",
    "warnings.filterwarnings('ignore')\n",
    "start_time = time.time()\n",
    "xgb_model = XGBClassifier(learning_rate =0.1,\n",
    "                             n_estimators=500,\n",
    "                             max_depth=5,\n",
    "                             min_child_weight=5,\n",
    "                             subsample=0.8,\n",
    "                             colsample_bytree=0.8,\n",
    "                             scale_pos_weight=1,\n",
    "                             random_state=10) \n",
    " \n",
    "\n",
    "parameters = {'n_jobs':[35],\n",
    "             'gamma':[i/10.0 for i in range(0,5)]\n",
    "             }\n",
    "\n",
    "clf = GridSearchCV(xgb_model, parameters,cv=3, verbose=0,n_jobs=1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gamma': 0.1, 'n_jobs': 35}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find Best Parameters\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.727625\n",
      "-- Execution time: 5729.693859577179 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Tune subsample and colsample_bytree\n",
    "# Subsample denotes the fraction of observates to be randomly sampled. Lower values make algorithm more conservative 2 small leads to underfitting\n",
    "# colsample_bytree denotes the fraction of coumns to be randomly sampled for each tree\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "start_time = time.time()\n",
    "xgb_model = XGBClassifier(learning_rate =0.1,\n",
    "                             n_estimators=500,\n",
    "                             max_depth=5,\n",
    "                             min_child_weight=5,\n",
    "                             gamma=0.1,\n",
    "                             scale_pos_weight=1,\n",
    "                             random_state=10) \n",
    " \n",
    "\n",
    "parameters = {'n_jobs':[200],\n",
    "             'subsample':[i/100.0 for i in range(75,90,5)],\n",
    "             'colsample_bytree':[i/100.0 for i in range(75,90,5)]\n",
    "             }\n",
    "\n",
    "clf = GridSearchCV(xgb_model, parameters,cv=3, verbose=0,n_jobs=1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.8, 'n_jobs': 200, 'subsample': 0.8}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find Best Parameters\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune the regularization parameter\n",
    "# reg_alpha is the L1 regularization on the weights. Can be used with high dimensionality so it runs faster.\n",
    "# reg_lambda is the L2 regularization on the weights\n",
    "# EC2 instance got terminated before had chance to submit. The L1 and L2 regularization was the default ones\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "start_time = time.time()\n",
    "\n",
    "xgb_model = XGBClassifier(learning_rate =0.1,\n",
    "                             n_estimators=500,\n",
    "                             max_depth=5,\n",
    "                             min_child_weight=5,\n",
    "                             gamma=0.1,\n",
    "                             subsample=0.8,\n",
    "                             colsample_bytree=0.8, \n",
    "                             scale_pos_weight=1,\n",
    "                             random_state=10) \n",
    " \n",
    "\n",
    "parameters = {'n_jobs':[-1],\n",
    "             'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05],\n",
    "             'reg_lambda':[0, 0.5, 1]\n",
    "             }\n",
    "\n",
    "clf = GridSearchCV(xgb_model, parameters,cv=3, verbose=0,n_jobs=1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Best Parameters\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune estimators and the learning parameter\n",
    "warnings.filterwarnings('ignore')\n",
    "start_time = time.time()\n",
    "xgb_model = XGBClassifier(   max_depth=5,\n",
    "                             min_child_weight=5,\n",
    "                             gamma=0.1,\n",
    "                             subsample=0.8,\n",
    "                             colsample_bytree=0.8, \n",
    "                             scale_pos_weight=1,\n",
    "                             random_state=10) \n",
    " \n",
    "\n",
    "parameters = {'n_jobs':[-1],\n",
    "             'n_estimators':range(500,5501,1000),\n",
    "             'learning_rate':[0.0001, 0.001, 0.01, 0.1, 0.2]\n",
    "             }\n",
    "\n",
    "clf = GridSearchCV(xgb_model, parameters,cv=3, verbose=0,n_jobs=1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"-- CV Score: \" + str(clf.best_score_))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizer Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# Create lemmatizer using spacy\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "\n",
    "def custom_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return([token.lemma_ for token in tokens if not token.is_punct])\n",
    "\n",
    "xgb_model = XGBClassifier(   max_depth=5,\n",
    "                             min_child_weight=5,\n",
    "                             gamma=0.1,\n",
    "                             subsample=0.8,\n",
    "                             colsample_bytree=0.8, \n",
    "                             scale_pos_weight=1,\n",
    "                             random_state=10,\n",
    "                             n_jobs=200) \n",
    "\n",
    "pipe = Pipeline(steps=[('vectidf', TfidfVectorizer(tokenizer=custom_tokenizer, stop_words='english',lowercase=True,use_idf=True,max_df=0.5)),\n",
    "                 ('svd', TruncatedSVD(500)),\n",
    "                 #('norm',Normalizer(copy=False)),\n",
    "                 ('xgb',xgb_model)\n",
    "                 ])\n",
    "\n",
    "parameters = {'vectidf__ngram_range': [(1, 1), (1, 2)],\n",
    "              'vectidf__min_df':(1,2),\n",
    "              'vectidf__norm':['l1','l2']\n",
    "              }\n",
    "\n",
    "grid = GridSearchCV(pipe, parameters, n_jobs=1, cv=3, verbose=0)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split between outcome and Features\n",
    "y = tweets['sentiment']\n",
    "X = tweets['Clean']\n",
    "\n",
    "#splitting into training and test sets even though still going to do k folds on the training data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "\n",
    "# Going to remove the normalizer step since the tfidfvectoizer already normalizes, and the model doesnt need it to be normalized.\n",
    "\n",
    "start_time = time.time()\n",
    "# Create lemmatizer using spacy\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "\n",
    "def custom_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return([token.lemma_ for token in tokens if not token.is_punct])\n",
    "\n",
    "xgb_model = XGBClassifier(   max_depth=5,\n",
    "                             min_child_weight=5,\n",
    "                             gamma=0.1,\n",
    "                             subsample=0.8,\n",
    "                             colsample_bytree=0.8, \n",
    "                             scale_pos_weight=1,\n",
    "                             random_state=10,\n",
    "                             n_jobs=-1) \n",
    "\n",
    "pipe = Pipeline(steps=[('vectidf', TfidfVectorizer(tokenizer=custom_tokenizer, stop_words='english',lowercase=True,use_idf=True,max_df=0.5)),\n",
    "                 ('svd', TruncatedSVD(2000)),\n",
    "                 ('xgb',xgb_model)\n",
    "                 ])\n",
    "\n",
    "parameters = {'vectidf__ngram_range': [(1, 1), (1, 2)],\n",
    "              'vectidf__min_df':(1,2),\n",
    "              'vectidf__norm':['l1','l2'],\n",
    "              'vectidf__smooth_idf':[True, False]\n",
    "              }\n",
    "# Pre dispatch controls the number of jobs that gets dispatched during parallel execution. \n",
    "grid = GridSearchCV(pipe, parameters, pre_dispatch=3, cv=3, verbose=0)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Check\n",
    "\n",
    "Want to see how the model handles unvectorized tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 60.7186336517334 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Down Sample\n",
    "tweets_subsampled_1, tweets_subsampled_2 = train_test_split(tweets, test_size=0.1)\n",
    "\n",
    "#Split between outcome and Features\n",
    "y = tweets_subsampled_2['sentiment']\n",
    "X = tweets_subsampled_2['Clean']\n",
    "\n",
    "# Split to Test Train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25)\n",
    "\n",
    "start_time = time.time()\n",
    "# Create lemmatizer using spacy\n",
    "lemmatizer = spacy.lang.en.English()\n",
    "\n",
    "def custom_tokenizer(doc):\n",
    "    tokens = lemmatizer(doc)\n",
    "    return([token.lemma_ for token in tokens if not token.is_punct])\n",
    "\n",
    "\n",
    "xgb_model = XGBClassifier(   max_depth=5,\n",
    "                             min_child_weight=5,\n",
    "                             gamma=0.1,\n",
    "                             subsample=0.8,\n",
    "                             colsample_bytree=0.8, \n",
    "                             scale_pos_weight=1,\n",
    "                             random_state=10,\n",
    "                             n_jobs=-1) \n",
    "\n",
    "pipe = Pipeline(steps=[('vectidf', TfidfVectorizer(tokenizer=custom_tokenizer, stop_words='english',\n",
    "                             lowercase=True, use_idf=True, max_df=0.5, max_features=1000,\n",
    "                             min_df=2, norm='l2', smooth_idf=True, ngram_range=(1, 2))),\n",
    "                 ('svd', TruncatedSVD(500)),\n",
    "                 ('xgb',xgb_model)\n",
    "                 ])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "753429                Dyinn , waitin on mother nature uggghh\n",
       "493063     Update on snake tweet: the copperhead has been...\n",
       "565646                           Allah yesms3 mink ya nawaal\n",
       "1463444                    is a virtual hug good enough too?\n",
       "536247                      Why is my throat still hurting?!\n",
       "Name: Clean, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "prediction = pipe.predict(X_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 4, 4, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
