{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Initialize ##################\n",
    "import findspark\n",
    "findspark.init('/usr/lib/spark')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('nlp').getOrCreate()\n",
    "import time\n",
    "\n",
    "# Feature Engineering\n",
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer,\n",
    "                                Tokenizer,StopWordsRemover, CountVectorizer,IDF,StringIndexer, HashingTF)\n",
    "from pyspark.sql.functions import length\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "import preprocessor as p\n",
    "\n",
    "# Models\n",
    "from pyspark.ml.classification import GBTClassifier,RandomForestClassifier, NaiveBayes, LogisticRegression\n",
    "\n",
    "# Pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Evaluators\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|sentiment|                text|\n",
      "+---------+--------------------+\n",
      "|        0|@switchfoot http:...|\n",
      "|        0|is upset that he ...|\n",
      "|        0|@Kenichan I dived...|\n",
      "|        0|my whole body fee...|\n",
      "|        0|@nationwideclass ...|\n",
      "+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############## Data ########################\n",
    "\n",
    "data = spark.read.csv(\"s3n://data-science-project-data/Twitter_Sentiment_Analysis/training.1600000.processed.noemoticon.csv\")\n",
    "data = data.withColumnRenamed('_c0','sentiment').withColumnRenamed('_c1','id').withColumnRenamed('_c2','date').withColumnRenamed('_c3','query_string').withColumnRenamed('_c4','user').withColumnRenamed('_c5','text')\n",
    "\n",
    "#Drop Data only need 2 columns\n",
    "data_dropped = data.select(['sentiment', 'text'])\n",
    "data_dropped.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+\n",
      "|sentiment|                text|          clean_text|\n",
      "+---------+--------------------+--------------------+\n",
      "|        0|@switchfoot http:...|- Awww, that's a ...|\n",
      "|        0|is upset that he ...|is upset that he ...|\n",
      "|        0|@Kenichan I dived...|I dived many time...|\n",
      "|        0|my whole body fee...|my whole body fee...|\n",
      "|        0|@nationwideclass ...|no, it's not beha...|\n",
      "+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "-- Execution time: 1.277602195739746 seconds ---\n"
     ]
    }
   ],
   "source": [
    "#################### Process Tweets ###############\n",
    "start_time = time.time()\n",
    "p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.MENTION, p.OPT.RESERVED, p.OPT.EMOJI, p.OPT.HASHTAG)\n",
    "def preprocess_tweet(tweet):\n",
    "    return p.clean(tweet)\n",
    "    \n",
    "clean_tweets = udf(lambda text :preprocess_tweet(text), StringType())\n",
    "\n",
    "clean_data = data_dropped.withColumn(\"clean_text\", clean_tweets(col(\"text\")))\n",
    "#clean_data = clean_data.withColumnRenamed('sentiment','label')\n",
    "\n",
    "clean_data.show(5)\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+\n",
      "|sentiment|                text|          clean_text|\n",
      "+---------+--------------------+--------------------+\n",
      "|        0|        my heart ...|my heart hurts ba...|\n",
      "|        0|     what the fuc...|what the fucccckk...|\n",
      "|        0|    I just cut my...|I just cut my bea...|\n",
      "|        0|    Not feeling i...|Not feeling it to...|\n",
      "|        0|       wompppp wompp|       wompppp wompp|\n",
      "+---------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Downsample to make it easier to deal with\n",
    "(downsample_1,downsample_2) = clean_data.randomSplit([0.2,0.8])\n",
    "\n",
    "# To avoid Data leakage split the data\n",
    "(training,testing) = downsample_1.randomSplit([0.7,0.3])\n",
    "\n",
    "training.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Execution time: 147.39291715621948 seconds ---\n"
     ]
    }
   ],
   "source": [
    "################ Transformation ##############################\n",
    "start_time = time.time()\n",
    "# Setup Transformations\n",
    "tokenizer = Tokenizer(inputCol=\"clean_text\", outputCol=\"token_text\")\n",
    "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
    "hashingTF = HashingTF(inputCol=\"stop_tokens\", outputCol=\"c_vec\", numFeatures=10000)\n",
    "#count_vec = CountVectorizer(inputCol='stop_tokens',outputCol='c_vec')\n",
    "idf = IDF(inputCol=\"c_vec\", outputCol=\"tf_idf\")\n",
    "sentiment_to_num = StringIndexer(inputCol='sentiment',outputCol='label')\n",
    "\n",
    "\n",
    "# Vectorize\n",
    "clean_up = VectorAssembler(inputCols=['tf_idf'],outputCol='features')\n",
    "\n",
    "# Build Pipeline\n",
    "data_prep_pipe = Pipeline(stages=[sentiment_to_num,tokenizer,stopremove,hashingTF,idf,clean_up])\n",
    "\n",
    "# Call Pipeline for training and testing\n",
    "\n",
    "#To prevent data leakage, transform the test data on the learned documents from training. \n",
    "#This is like the real world where only have access to the training data.\n",
    "cleaner = data_prep_pipe.fit(training)\n",
    "training_cleaner = cleaner.transform(training)\n",
    "testing_cleaner = cleaner.transform(testing)\n",
    "\n",
    "# Select Clean Data\n",
    "train_clean_data = training_cleaner.select(['label','features'])\n",
    "test_clean_data = testing_cleaner.select(['label','features'])\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting sentiment was: 0.7187903969369565\n",
      "-- Execution time: 187.50895714759827 seconds ---\n"
     ]
    }
   ],
   "source": [
    "####### Naive Bayes ##########\n",
    "start_time = time.time()\n",
    "nb = NaiveBayes()\n",
    "\n",
    "# Fit model\n",
    "naive_model = nb.fit(train_clean_data)\n",
    "\n",
    "# Evaluate the model\n",
    "test_results = naive_model.transform(test_clean_data)\n",
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print(\"Accuracy of model at predicting sentiment was: {}\".format(acc))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting spam was: 0.729032724383591\n",
      "-- Execution time: 155.02955102920532 seconds ---\n"
     ]
    }
   ],
   "source": [
    "######## Logistic Regression ######\n",
    "start_time = time.time()\n",
    "# Setup Model\n",
    "log_reg = LogisticRegression()\n",
    "log_model = log_reg.fit(train_clean_data)\n",
    "\n",
    "# Evaluate the model\n",
    "test_results = log_model.transform(test_clean_data)\n",
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print(\"Accuracy of model at predicting spam was: {}\".format(acc))\n",
    "print(\"-- Execution time: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
